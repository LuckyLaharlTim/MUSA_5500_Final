{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries \n",
    "\n",
    " - Request library allows you to send HTTP request in python to a specific URL. In our case we send an HTTP request to Zillow\n",
    " - Time module allows to handle time related task including formatting dates, waiting and representing time\n",
    " - The random module allows you to generate random \n",
    " - The bs4 module allows you to pull data from HTML document after you get a response from HTTP request\n",
    " - The os modules allows ou to interact with operating systems including changing working directory\n",
    " - The selenium module allows you to automate interaction with a web browser including sending URL request and extracting HTML\n",
    "   document response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from random import sample \n",
    "from random import randint\n",
    "import pandas as pd \n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Path\n",
    " - Identify your destination folder\n",
    " - Use os change directory to set your destination directory as the default. That is where all outputs will be exported to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../webscraping_outputs-C\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set URL \n",
    "- https:// yourcityname.craigslist.org/search/apa\n",
    "- This URL is created to search for the number of pages in a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://charlotte.craigslist.org/search/apa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Headers\n",
    "\n",
    "- We set headers here because with headers, Craigslist without will know we a webscraping their data and block our request. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language' : 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'cookie': 'cl_b=4|51d3ac35358ee42ae349258960e058830d03e113|1633897219_XNdY; cl_def_hp=charlotte',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-stie': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:87.0) Gecko/20100101 Firefox/87.0'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Response\n",
    "- We test response from our request, as long as it returns 200, we know the resquest is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers = headers)\n",
    "response\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"cl-content\">\n",
       " <main>\n",
       " </main>\n",
       " </div>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(response.text,'html.parser').findAll('div',class_=\"cl-content\")\n",
    "# craigslist has to be changed all over to get same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Start: 2023-11-13 11:40:25\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\timol\\Grad_School\\Fall_2023\\Geospatial_Data_Science_in_Python\\final_project\\providence_adu_Code\\RentalExclusionaryCriteria\\Webscraping Craigslist Data.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m results_num \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msearch-legend\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m results_total \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(results_num\u001b[39m.\u001b[39;49mfind(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtotalcount\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pages \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, results_total\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m120\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Create an outfile file name, I called mine FinalCraiglist and formatted it a date time stamp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Note: If you are scraping multiple times in a day, then you need to format the time stamp with hours \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/timol/Grad_School/Fall_2023/Geospatial_Data_Science_in_Python/final_project/providence_adu_Code/RentalExclusionaryCriteria/Webscraping%20Craigslist%20Data.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# that way you don't overwrite already exported data\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# This step reports the time code is intialized \n",
    "print(\"Script Start: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# In this step we identified the total number of pages for all the rental listings on craiglist in our city\n",
    "# to do this we create a beautiful soup object from our request response\n",
    "# we then use the find method to search through the div class for the class id \"search-legend\"\n",
    "# under the 'search-legend' class we search for 'totalcount' which has the total number of pages\n",
    "# After finding the total number of pages (in our case 3000),\n",
    "# we create a range values between 0 and 3000 with a step of 120 since there are listing on each page\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "results_num = soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text)\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "# Create an outfile file name, I called mine FinalCraiglist and formatted it a date time stamp\n",
    "# Note: If you are scraping multiple times in a day, then you need to format the time stamp with hours \n",
    "# that way you don't overwrite already exported data\n",
    "\n",
    "finalfile = \"FinalCraiglist\" + \"_\" + \"{:%Y_%m_%d %H:%M}\".format(datetime.now()) +\".csv\"\n",
    "\n",
    "# We create a list that holds the results\n",
    "\n",
    "result = []\n",
    "\n",
    "# We loop through each of the pages and append the number at the end of the search URL \n",
    "\n",
    "for page in pages:\n",
    "    print(page)\n",
    "    \n",
    "    # The URL inlcude the name of the city in this case charlotte, and appended the page number as string\n",
    "    # for example 'https://yourcity.craigslist.org/d/apartments-housing-for-rent/search/apa?'+\"s=\" +str(page)\n",
    "    \n",
    "    url = 'https://charlotte.craigslist.org/d/apartments-housing-for-rent/search/apa?'+\"s=\" +str(page)\n",
    "    \n",
    "    # If we overload the Craigslist server with repeated request, they will block us so we set random wait \n",
    "    # with the time module \n",
    "    time.sleep(randint(0, 5))\n",
    "    \n",
    "    # We send a request to the URL and pass our headers as an additional argument \n",
    "    response = requests.get(url, headers = headers)\n",
    "    response\n",
    "    \n",
    "    # We create a beautifulsoup object for our request response \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # We search through the beautiful soup object using the find method for the class id \"result-row\"\n",
    "    # The 'result-row' is a list of all the rental listings on each page\n",
    "    \n",
    "    posts  = soup.find_all('li', class_= 'result-row')\n",
    "    time.sleep(randint(0, 2))\n",
    "   \n",
    "    # We loop through each listings to extract the URL associated with each individual listings\n",
    "    # this would allow us to access listing specific URL \n",
    "    \n",
    "    for post in posts:\n",
    "        \n",
    "        # We use the find method to search for the class id 'result-title hdrlnk', \n",
    "        # this id stores the URL for each rental listings \n",
    "        \n",
    "        listingURL = post.find('a', class_='result-title hdrlnk')['href']\n",
    "        time.sleep(randint(0, 5))\n",
    "        \n",
    "        # We pass the URL of each listing as a request, and create a beautiful soup object for the results\n",
    "        \n",
    "        responserent = requests.get(listingURL, headers = headers)\n",
    "        renthtml = BeautifulSoup(responserent.text, 'html.parser')\n",
    "        \n",
    "        # We extract the variables for each rental listings. \n",
    "        # For rental price, date of rental posting, neighborhood of rental listing, and rental URL\n",
    "        # we can extract them for general html document\n",
    "        # for the listing address, description text, number of bathrooms, latitude, longitude and floor size\n",
    "        # we extract them from rental listing specific html document\n",
    "        \n",
    "        # We append the results to a list \n",
    "        \n",
    "        try:\n",
    "            result.append({\n",
    "                \n",
    "            'Post_datetime':post.find('time', class_= 'result-date')['datetime'],    \n",
    "            'Post Price': renthtml.find('span',{'class': 'price'}).text,\n",
    "            'Post hood': post.find('span',class_ = 'result-hood'),\n",
    "            'Post title': renthtml.find('span',{'id': 'titletextonly'}).text,\n",
    "            'PostURL': post.find('a', class_='result-title hdrlnk')['href'],\n",
    "            'address':  renthtml.find('div', {'class': 'mapaddress'}).text,\n",
    "            'overview': renthtml.find('section', {'id': 'postingbody'}),\n",
    "            'bedroom_baths':renthtml.find('span', {'class': 'shared-line-bubble'}).text,\n",
    "            'Title': renthtml.find('span',{'id':'titletextonly'}).text,\n",
    "            'Price': renthtml.find('span',{'class': 'price'}).text,\n",
    "            'lat': renthtml.find('div',{'class':'viewposting'})['data-latitude'],\n",
    "            'lon': renthtml.find('div',{'class':'viewposting'})['data-longitude'],\n",
    "            'Other':renthtml.find('p', {'class': 'attrgroup'})\n",
    "\n",
    "                \n",
    "            })\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # The results are converted to a pandas dataframe, and exported as CSV file \n",
    "   \n",
    "        Craiglistdata = pd.DataFrame(result)\n",
    "        Craiglistdata.to_csv(finalfile, index=False)\n",
    "        \n",
    "print(\"Script End: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
