{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: Timothy Oliver (modified from Providence Adu)\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "        page-layout: full\n",
    "        code-fold:  false\n",
    "execute:\n",
    "    warning:    false\n",
    "    message:    false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-scraping Zillow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-scraping at Given Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries \n",
    "\n",
    " - Request library allows you to send HTTP request in python to a specific URL. In our case we send an HTTP request to Zillow\n",
    " - Time module allows to handle time related task including formatting dates, waiting and representing time\n",
    " - The random module allows you to generate random \n",
    " - The bs4 module allows you to pull data from HTML document after you get a response from HTTP request\n",
    " - The os modules allows ou to interact with operating systems including changing working directory\n",
    " - The selenium module allows you to automate interaction with a web browser including sending URL request and extracting HTML\n",
    "   document response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from random import sample \n",
    "import pandas as pd \n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import glob as gb \n",
    "from selenium_stealth import stealth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Path\n",
    " - Identify your destination folder\n",
    " - Use os change directory to set your destination directory as the default. That is where all outputs will be exported to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../webscraping_outputs-Z\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a file name\n",
    " - Create an outfile file name, I called mine ZillowSelium and formatted it with a date time stamp of the current time down to the current second\n",
    "    - Note: If you are scraping multiple times in a day, then you need to format the time stamp with hours or more minutely so that you don't overwrite already exported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZillowSelenium_2023_Dec_21_13-45-20.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalfile = \"ZillowSelenium\" + \"_\" + \"{:%Y_%h_%d_%H-%M-%S}\".format(datetime.now()) +\".csv\"\n",
    "finalfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Webscraping \n",
    "\n",
    "- sets up Selenium webdrivers with `selenium-stealth` and searches for specific html tag classes\n",
    "    -   It is recommended to check this every so often should tags or their classes change\n",
    "- Outputs results of obtained realtor pages and successfully obtained information\n",
    "- Output written to file with the `finalfile` name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#Create a list that will hold the results\n",
    "\n",
    "page_count=20\n",
    "results = []\n",
    "\n",
    "zillow_placeholders = [\"This property accepts Zillow applications\",\"Zillow last checked:\"]\n",
    "\n",
    "# selectors\n",
    "titleSelector = \"h1.Text-c11n-8-84-3__sc-aiai24-0\"\n",
    "descSelector = \"div.building-description\"\n",
    "linkSelector = \"div.StyledPropertyCardDataWrapper-c11n-8-84-3__sc-1omp4c3-0\"\n",
    "saleSelector = \"div.Text-c11n-8-84-3__sc-aiai24-0\"\n",
    "sale2Selector = \"p.Text-c11n-8-84-3__sc-aiai24-0\"\n",
    "nhoodSelector = \"h4.Text-c11n-8-84-3__sc-aiai24-0\"\n",
    "nhood2Selector = \"h2.styledComponents__BuildingCardTitle-sc-1bj2ydz-8\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "textDriver = webdriver.Chrome()\n",
    "\n",
    "stealth(driver,\n",
    "       languages=[\"en-US\", \"en\"],\n",
    "       vendor=\"Google Inc.\",\n",
    "       platform=\"Win32\",\n",
    "       webgl_vendor=\"Intel Inc.\",\n",
    "       renderer=\"Intel Iris OpenGL Engine\",\n",
    "       fix_hairline=True,\n",
    "       )\n",
    "\n",
    "stealth(textDriver,\n",
    "       languages=[\"en-US\", \"en\"],\n",
    "       vendor=\"Google Inc.\",\n",
    "       platform=\"Win32\",\n",
    "       webgl_vendor=\"Intel Inc.\",\n",
    "       renderer=\"Intel Iris OpenGL Engine\",\n",
    "       fix_hairline=True,\n",
    "       )\n",
    "\n",
    "url =\"https://www.zillow.com/philadelphia-pa/rentals/\"\n",
    "\n",
    "url2 = \"https://www.zillow.com/philadelphia-pa/for_sale/\"\n",
    "\n",
    "# Inspect the zillow website and figure out the number pages for rental ads use\n",
    "# In the charlotte example, there are a total of 20 pages so I set the range at 21\n",
    "\n",
    "for page in range(1,page_count+1,1):\n",
    "    \n",
    "    print(\"This is page: \" + str(page))\n",
    "    \n",
    "    #Identify the Zillow URL of your City, it should follow this format:\n",
    "    # 1. Default Zillow url : https://www.zillow.com/\n",
    "    # 2. Name of your City: eg. charlotte-nc, atlanta-ga\n",
    "    # 3. Pass the page number \n",
    "    # 4. Add the \"_p\" that is a default thing with the Zillow website \n",
    "    # 5. In a sample URL on page 15 for example will be like: https://www.zillow.com/charlotte-nc/rentals/15_p/\n",
    "\n",
    "    page = str(page) + '_p/'\n",
    "    \n",
    "    # Here we are going to utilize the selenium. To automate the interaction behavior of a web browser you would\n",
    "    # need a web driver. Each browser has a webdriver, in my case I am using google chrome so I download the web driver\n",
    "    # from this website \"https://chromedriver.storage.googleapis.com/index.html?path=98.0.4758.80/\" \n",
    "    \n",
    "    # After downloading and extracting the web drive(chromdriver.exe) you use the webdrive.Chrome() method to initiate\n",
    "    # the chrome browser and pass the path where the driver is saved.\n",
    "    \n",
    "    \n",
    "    # CraiglistBrowser.maximize_window()\n",
    "\n",
    "    # After the browser has been launched use the get() to pass the url \n",
    "    print(f\"Urls:\\n\")\n",
    "    page_links = []\n",
    "    for url in [url,url2]: # getListingType():\n",
    "        print(f\"\\t\\t{url+page}\\n\")\n",
    "        browser = driver.get(url+page)\n",
    "        html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        for item in soup.select(linkSelector):\n",
    "            l = item.select(\"a\")[0].attrs[\"href\"]\n",
    "            if not(l.startswith(\"https://\")):\n",
    "                l = \"https://www.zillow.com\"+l\n",
    "            page_links.append(l)\n",
    "\n",
    "    for link in page_links:\n",
    "        ovPage = textDriver.get(link)\n",
    "        textSoup = BeautifulSoup(textDriver.page_source,\"html.parser\")\n",
    "\n",
    "        if len(textSoup.select(\"div.px-captcha-container\")) > 0:\n",
    "            time.sleep(0.3)\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            title = textSoup.select(titleSelector)[0].text\n",
    "            nh1 = textSoup.select(nhoodSelector)\n",
    "            nh2 = textSoup.select(nhood2Selector)\n",
    "            nhood = None\n",
    "\n",
    "            # get neighborhood from among header tags\n",
    "            if len(nh1) > 0:\n",
    "                for blurb in nh1:\n",
    "                    if \"neighborhood:\" in blurb.text.lower():\n",
    "                        nhood = blurb.text.split(\":\")[1][1:]\n",
    "                        # print(blurb.text.split(\":\")[1][1:])\n",
    "                        break\n",
    "            elif (len(nh2) > 0 and type(nhood) == type(None)):\n",
    "                for blurb in nh2:\n",
    "                    if \"neighborhood:\" in blurb.text.lower():\n",
    "                        nhood = blurb.text.split(\":\")[1][1:]\n",
    "                        # print(blurb.text.split(\":\")[1][1:])\n",
    "                        break\n",
    "\n",
    "            # getting address from title\n",
    "            address = None\n",
    "            for w in range(len(title)):\n",
    "                if title[w].isnumeric():\n",
    "                    address = title[w:]\n",
    "                    break\n",
    "\n",
    "            if len(textSoup.select(descSelector))>0 and len(textSoup.select(descSelector)[0].text)>70 and (not(any(holder in textSoup.select(descSelector)[0].text for holder in zillow_placeholders))):\n",
    "                text = textSoup.select(descSelector)[0].text\n",
    "            elif len(textSoup.select(sale2Selector)[0]) and len(textSoup.select(sale2Selector)[0].text)>70 and (not(any(holder in textSoup.select(sale2Selector)[0].text for holder in zillow_placeholders))):\n",
    "                text= textSoup.select(sale2Selector)[0].text\n",
    "            elif len(textSoup.select(saleSelector)[0])>0 and len(textSoup.select(saleSelector)[0].text)>70 and (not(any(holder in textSoup.select(saleSelector)[0].text for holder in zillow_placeholders))):\n",
    "                text= textSoup.select(saleSelector)[0].text\n",
    "            else:\n",
    "                text=\"\"\n",
    "\n",
    "            results.append({\n",
    "            \"title\": title,\n",
    "            \"address\": address,\n",
    "            \"neighborhood\": nhood,\n",
    "            \"description\": text,\n",
    "            \"url\": link\n",
    "            })\n",
    "            print(f\"title: {title}\\t\\taddress: {address}\\t\\tneighborhood: {nhood}\\nlink: {link}\\n\\tdescription: {text}\")\n",
    "\n",
    "            time.sleep(0.3)\n",
    "           \n",
    "\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "Zillowdata =  pd.DataFrame(results)\n",
    "Zillowdata.to_csv(finalfile, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save current results to file if ending prematurely due to error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "Zillowdata =  pd.DataFrame(results)\n",
    "Zillowdata.to_csv(finalfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of all CSV outputs \n",
    "- Use the glob method get generate a list of all your output csv in your directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/webscraping_outputs-Z\"\n",
    "os.chdir(path)\n",
    "All = gb.glob(path + \"/*.csv\")\n",
    "All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate CSV\n",
    "- combine all files as a single CSV by concatenating them. To do this:\n",
    "    - loop through the list of CSV in your path and use the pandas.read_csv method to read them. It will create a generator\n",
    "    - Afterward use the pd.concat method to concatenate all your csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(All)>0:\n",
    "    Zillow = (pd.read_csv(file) for file in All)  \n",
    "    FinalZillow   = pd.concat(Zillow, ignore_index=True)\n",
    "else:\n",
    "    import warnings\n",
    "    warnings.warn(f\"There are no data in {path}, try using 'Webscraping Zillow Data.ipynb'.\",UserWarning,stacklevel=2)\n",
    "\n",
    "FinalUnique = FinalZillow.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change directory for the output file and export the final output as a CSV file\n",
    " -  Change the output directory for your final CSV\n",
    " -  Since the cleaning (including concatenating) will be done multiple times for any addtional webscraping, you would have to \n",
    "    1.  export the final clean file to a different folder\n",
    "    \n",
    "    OR\n",
    "\n",
    "    2.  delete the old version using the commented portion in the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = path+\"/clean\"\n",
    "os.chdir(outPath)\n",
    "# if os.path.exists(\"ZillowUnique.csv\"):\n",
    "#     os.remove((\"ZillowUnique.csv\"))\n",
    "FinalUnique.to_csv(\"ZillowUnique.csv\", index=False,)\n",
    "(print(\"compilation complete\"))\n",
    "os.chdir(\"../\"+path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
